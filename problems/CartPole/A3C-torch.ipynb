{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import gym\r\n",
    "import torch\r\n",
    "import numpy as np\r\n",
    "import torch.multiprocessing as mp\r\n",
    "\r\n",
    "import jdc"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "env_test = gym.make(\"CartPole-v1\")\r\n",
    "env_test.observation_space, env_test.action_space.n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(Box(-3.4028234663852886e+38, 3.4028234663852886e+38, (4,), float32), 2)"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## A3C"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Fully-Connected Policy Action Network"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "## Fully-Connected Policy Action Network\r\n",
    "class FCPA(torch.nn.Module):\r\n",
    "    def __init__(self, input_shape, output_shape, hidden_layers,\r\n",
    "                activation_fn=torch.nn.functional.relu,\r\n",
    "                optimizer=torch.optim.Adam, learning_rate=0.0005,\r\n",
    "                grad_max_norm=1, entropy_loss_weight=0.001) -> None:\r\n",
    "        super().__init__()\r\n",
    "        self.grad_max_norm = grad_max_norm\r\n",
    "        self.entropy_loss_weight = entropy_loss_weight\r\n",
    "        self.activation_fn = activation_fn\r\n",
    "\r\n",
    "        self.input_layer = torch.nn.Linear(input_shape, hidden_layers[0])\r\n",
    "        self.hidden_layers = torch.nn.ModuleList()\r\n",
    "        for i in range(len(hidden_layers)-1):\r\n",
    "            self.hidden_layers.append(torch.nn.Linear(hidden_layers[i], hidden_layers[i+1]))\r\n",
    "        self.output_layer = torch.nn.Linear(hidden_layers[-1], output_shape)\r\n",
    "\r\n",
    "        self.optimizer = optimizer(self.parameters(), lr=learning_rate)\r\n",
    "        self.optimizer.zero_grad()\r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "        if not isinstance(x, torch.Tensor):\r\n",
    "            x = torch.tensor(x, dtype=torch.float32)\r\n",
    "        x = self.activation_fn(self.input_layer(x))\r\n",
    "        for hidden_layer in self.hidden_layers:\r\n",
    "            x = self.activation_fn(hidden_layer(x))\r\n",
    "        return self.output_layer(x)\r\n",
    "\r\n",
    "    def softmax_policy(self, state):\r\n",
    "        logits = self(state)\r\n",
    "        dist = torch.distributions.Categorical(logits=logits)\r\n",
    "        action = dist.sample().item()\r\n",
    "        return action\r\n",
    "\r\n",
    "    def greedy_policy(self, state):\r\n",
    "        tens_state = torch.tensor(state).unsqueeze(dim=0)\r\n",
    "        logits = self(tens_state).detach().numpy()\r\n",
    "        return np.argmax(logits)\r\n",
    "\r\n",
    "    def reset(self):\r\n",
    "        self.apply(FCPA.reset_weights)\r\n",
    "\r\n",
    "    @staticmethod\r\n",
    "    def reset_weights(m):\r\n",
    "        for layer in m.children():\r\n",
    "            if hasattr(layer, 'reset_parameters'):\r\n",
    "                layer.reset_parameters()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Fully-Connected Value Network"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "## Fully-Connected Value Network\r\n",
    "class FCV(torch.nn.Module):\r\n",
    "    def __init__(self, input_shape, hidden_layers,\r\n",
    "                activation_fn=torch.nn.functional.relu,\r\n",
    "                optimizer=torch.optim.Adam, learning_rate=0.0005,\r\n",
    "                grad_max_norm=float(\"inf\")) -> None:\r\n",
    "        super().__init__()\r\n",
    "        self.grad_max_norm = grad_max_norm\r\n",
    "        self.activation_fn = activation_fn\r\n",
    "\r\n",
    "        self.input_layer = torch.nn.Linear(input_shape, hidden_layers[0])\r\n",
    "        self.hidden_layers = torch.nn.ModuleList()\r\n",
    "        for i in range(len(hidden_layers)-1):\r\n",
    "            self.hidden_layers.append(torch.nn.Linear(hidden_layers[i], hidden_layers[i+1]))\r\n",
    "        self.output_layer = torch.nn.Linear(hidden_layers[-1], 1)\r\n",
    "\r\n",
    "        self.optimizer = optimizer(self.parameters(), lr=learning_rate)\r\n",
    "        self.optimizer.zero_grad()\r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "        if not isinstance(x, torch.Tensor):\r\n",
    "            x = torch.tensor(x, dtype=torch.float32)\r\n",
    "        x = self.activation_fn(self.input_layer(x))\r\n",
    "        for hidden_layer in self.hidden_layers:\r\n",
    "            x = self.activation_fn(hidden_layer(x))\r\n",
    "        return self.output_layer(x)\r\n",
    "\r\n",
    "    @staticmethod\r\n",
    "    def reset_weights(m):\r\n",
    "        for layer in m.children():\r\n",
    "            if hasattr(layer, 'reset_parameters'):\r\n",
    "                layer.reset_parameters()\r\n",
    "\r\n",
    "    def reset(self):\r\n",
    "        self.apply(FCV.reset_weights)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "class Buffer:\r\n",
    "    def __init__(self) -> None:\r\n",
    "        self.states = []\r\n",
    "        self.rewards = []\r\n",
    "        self.actions = []\r\n",
    "\r\n",
    "    def store(self, state, action, reward):\r\n",
    "        self.states.append(state)\r\n",
    "        self.rewards.append(reward)\r\n",
    "        self.actions.append(action)\r\n",
    "\r\n",
    "    def clear(self):\r\n",
    "        self.states = []\r\n",
    "        self.rewards = []\r\n",
    "        self.actions = []\r\n",
    "\r\n",
    "    def get(self):\r\n",
    "        return self.states, self.actions, self.rewards"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "def create_env():\r\n",
    "    return gym.make(\"CartPole-v1\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "class SharedAdam(torch.optim.Adam):\r\n",
    "    def __init__(self, params, lr=1e-3,\r\n",
    "                 betas=(0.9, 0.999), eps=1e-8, weight_decay=0, amsgrad=False):\r\n",
    "        super().__init__(params, lr=lr, betas=betas, eps=eps, \r\n",
    "                         weight_decay=weight_decay, amsgrad=amsgrad)\r\n",
    "        for group in self.param_groups:\r\n",
    "            for p in group['params']:\r\n",
    "                state = self.state[p]\r\n",
    "                state['step'] = 0\r\n",
    "                state['shared_step'] = torch.zeros(1).share_memory_()\r\n",
    "                state['exp_avg'] = torch.zeros_like(p.data).share_memory_()\r\n",
    "                state['exp_avg_sq'] = torch.zeros_like(p.data).share_memory_()\r\n",
    "                if weight_decay:\r\n",
    "                    state['weight_decay'] = torch.zeros_like(p.data).share_memory_()\r\n",
    "                if amsgrad:\r\n",
    "                    state['max_exp_avg_sq'] = torch.zeros_like(p.data).share_memory_()\r\n",
    "\r\n",
    "    def step(self, closure=None):\r\n",
    "        for group in self.param_groups:\r\n",
    "            for p in group['params']:\r\n",
    "                if p.grad is None:\r\n",
    "                    continue\r\n",
    "                self.state[p]['steps'] = self.state[p]['shared_step'].item()\r\n",
    "                self.state[p]['shared_step'] += 1\r\n",
    "        super().step(closure)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "\r\n",
    "class Worker(mp.Process):\r\n",
    "    def __init__(self, id, global_T, create_env, fcpa, fcv, buffer,\r\n",
    "                global_policy_nn, global_value_nn, \r\n",
    "                shared_policy_optimizer, shared_value_optimizer, n_step_max, gamma=0.99, T_max=3000):\r\n",
    "        super().__init__()\r\n",
    "        self.id = id\r\n",
    "        self.env = create_env()\r\n",
    "        self.n_actions = self.env.action_space.n\r\n",
    "        \r\n",
    "        self.buffer = buffer\r\n",
    "        self.policy_nn = fcpa\r\n",
    "        self.value_nn = fcv\r\n",
    "        self.policy_nn.load_state_dict(global_policy_nn.state_dict())\r\n",
    "        self.value_nn.load_state_dict(global_value_nn.state_dict())\r\n",
    "\r\n",
    "        self.global_policy_nn = global_policy_nn\r\n",
    "        self.global_value_nn = global_value_nn\r\n",
    "        self.shared_policy_optimizer = shared_policy_optimizer\r\n",
    "        self.shared_value_optimizer = shared_value_optimizer\r\n",
    "\r\n",
    "        self.T = global_T\r\n",
    "        self.T_MAX = T_max\r\n",
    "        self.n_step_max = n_step_max\r\n",
    "        self.gamma= gamma"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "%%add_to Worker\r\n",
    "\r\n",
    "def get_returns(self, rewards, value_p):\r\n",
    "    n = len(rewards)\r\n",
    "    R = value_p\r\n",
    "    returns = np.zeros(n)\r\n",
    "    for i in range(n):\r\n",
    "        R = rewards[n-1-i] + self.gamma * R\r\n",
    "        returns[n-1-i] = R\r\n",
    "    return returns"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "%%add_to Worker\r\n",
    "\r\n",
    "def optimize(self, states, actions, returns):\r\n",
    "    T = len(returns)\r\n",
    "    discounts = torch.tensor(\r\n",
    "        np.logspace(0, T, num=T, base=self.gamma, endpoint=False)).unsqueeze(dim=1)\r\n",
    "    returns = torch.tensor(returns, dtype=torch.float).unsqueeze(dim=1)\r\n",
    "    actions = torch.tensor(actions, dtype=torch.float)\r\n",
    "\r\n",
    "    values = self.value_nn(states)\r\n",
    "    logits = self.policy_nn(states)\r\n",
    "\r\n",
    "    dist = torch.distributions.Categorical(logits=logits)\r\n",
    "    logpas = dist.log_prob(actions)\r\n",
    "    entropies = dist.entropy()\r\n",
    "\r\n",
    "    td_errors = returns - values\r\n",
    "\r\n",
    "    ###### policy optimization\r\n",
    "    p_loss = -torch.mean(discounts * logpas * td_errors.detach())\r\n",
    "    entropy_loss = -self.policy_nn.entropy_loss_weight * entropies.mean()\r\n",
    "    policy_loss = p_loss +  entropy_loss\r\n",
    "\r\n",
    "    self.shared_policy_optimizer.zero_grad()\r\n",
    "    policy_loss.backward()\r\n",
    "    torch.nn.utils.clip_grad_norm_(\r\n",
    "        self.policy_nn.parameters(), self.policy_nn.grad_max_norm)\r\n",
    "\r\n",
    "    for param, global_param in zip(self.policy_nn.parameters(),\r\n",
    "                                self.global_policy_nn.parameters()):\r\n",
    "        if global_param.grad is None:\r\n",
    "            global_param._grad = param.grad\r\n",
    "\r\n",
    "    self.shared_policy_optimizer.step()\r\n",
    "    self.policy_nn.load_state_dict(self.global_policy_nn.state_dict())\r\n",
    "\r\n",
    "    ###### value optimization\r\n",
    "    value_loss = torch.mean(1/2 * td_errors**2)\r\n",
    "    \r\n",
    "    self.shared_value_optimizer.zero_grad()\r\n",
    "    value_loss.backward()\r\n",
    "    torch.nn.utils.clip_grad_norm_(\r\n",
    "        self.value_nn.parameters(), self.value_nn.grad_max_norm)\r\n",
    "\r\n",
    "    for param, global_param in zip(self.value_nn.parameters(),\r\n",
    "                                self.global_value_nn.parameters()):\r\n",
    "        if global_param.grad is None:\r\n",
    "            global_param._grad = param.grad\r\n",
    "        \r\n",
    "    self.shared_value_optimizer.step()\r\n",
    "    self.value_nn.load_state_dict(self.global_value_nn.state_dict())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "%%add_to Worker\r\n",
    "\r\n",
    "def run(self):\r\n",
    "    t = 1\r\n",
    "    while self.T.value <= self.T_MAX:\r\n",
    "        sum_rewards = 0\r\n",
    "        t_start = t\r\n",
    "        state = self.env.reset()\r\n",
    "        while True:\r\n",
    "            action = self.policy_nn.softmax_policy(state)\r\n",
    "            state_p, reward, done, info = self.env.step(action)\r\n",
    "            sum_rewards += reward\r\n",
    "            is_truncated = 'TimeLimit.truncated' in info and info['TimeLimit.truncated']\r\n",
    "            is_failure = done and not is_truncated\r\n",
    "            self.buffer.store(state, action, reward)\r\n",
    "            if done or t - t_start == self.n_step_max:\r\n",
    "                states, actions, rewards = self.buffer.get()\r\n",
    "                if is_failure:\r\n",
    "                    value_p = 0\r\n",
    "                else:\r\n",
    "                    value_p = self.value_nn(state_p[np.newaxis])\r\n",
    "                returns = self.get_returns(rewards, value_p)\r\n",
    "                self.optimize(states, actions, returns)\r\n",
    "                self.buffer.clear()\r\n",
    "                t_start = t\r\n",
    "            if done: break\r\n",
    "            t += 1\r\n",
    "            state = state_p\r\n",
    "        with self.T.get_lock():\r\n",
    "            self.T.value += 1\r\n",
    "            \r\n",
    "        print(self.id, 'episode ', self.T.value, 'reward %.1f' % sum_rewards)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "class A3C:\r\n",
    "    def __init__(self, create_env, FCPA, FCV, Buffer, Worker,\r\n",
    "                 n_step_max=50, max_episodes=3000,\r\n",
    "                 policy_lr=0.0005, value_lr=0.0007):\r\n",
    "        self.create_env = create_env\r\n",
    "        self.Worker = Worker\r\n",
    "        self.FCPA = FCPA\r\n",
    "        self.FCV = FCV\r\n",
    "        self.Buffer = Buffer\r\n",
    "\r\n",
    "        self.policy_lr = policy_lr\r\n",
    "        self.value_lr = value_lr\r\n",
    "\r\n",
    "        self.global_policy_nn = FCPA(4, 2, (128,64)).share_memory()\r\n",
    "        self.global_value_nn = FCV(4, (256,128)).share_memory()\r\n",
    "        self.shared_policy_optimizer = SharedAdam(self.global_policy_nn.parameters(), lr=policy_lr)\r\n",
    "        self.shared_value_optimizer = SharedAdam(self.global_value_nn.parameters(), lr=value_lr)\r\n",
    "\r\n",
    "        self.max_episodes = max_episodes\r\n",
    "        self.n_step_max = n_step_max\r\n",
    "        self.global_T = mp.Value('i', 0)\r\n",
    "        \r\n",
    "\r\n",
    "    def train(self, n_workers):\r\n",
    "        workers = []\r\n",
    "        for i in range(n_workers):\r\n",
    "            worker = self.Worker(i, self.global_T, self.create_env, \r\n",
    "                                 self.FCPA(4, 2, (128,64)), self.FCV(4, (256,128)), self.Buffer(),\r\n",
    "                                 self.global_policy_nn, self.global_value_nn,\r\n",
    "                                 self.shared_policy_optimizer, self.shared_value_optimizer,\r\n",
    "                                 self.n_step_max)\r\n",
    "            workers.append(worker)\r\n",
    "\r\n",
    "        [w.start() for w in workers]\r\n",
    "        [w.join() for w in workers]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "agent = A3C(create_env, FCPA, FCV, Buffer, Worker, n_step_max=5)\r\n",
    "agent.train(8)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.9",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit"
  },
  "interpreter": {
   "hash": "c2e208dfd978d16ff671f46f93e2bb9e9317cbee21547e52ad1d928d29a01bdc"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}